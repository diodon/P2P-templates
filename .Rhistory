taxonRank = Rank)
## Event Measurement or Fact (eMOF) file
IPT.mof = data.frame(eventID = DF.data.noSubstrate$UNIT_ID,
occurrenceID = DF.data.noSubstrate$occurrenceID,
measurementType = tolower(DF.data.noSubstrate$Variable),
measurmenetTypeID = DF.data.noSubstrate$measurementTypeID,
measurementValue = DF.data.noSubstrate$Value,
measurementUnit = DF.data.noSubstrate$measurementUnit,
measurementUnitID = DF.data.noSubstrate$measurementUnitID
)
## reformat to wide
DF.dataWide = dcast(occurrenceID+LOCALITY+SITE+STRATA+SAMPLE+scientificName+AphiaID+Rank~Variable, value.var = "Value", data=DF.data, sum)
rootFileName = paste(unique(DF.sites$countryCodeISO), paste0(unique(DF.sites$localityCode, collapse="-")),
unique(DF.sites$HABITAT), gsub("-","", min(DF.sites$eventDate)), sep="_")
## IPT files
readr::write_csv(IPT.event, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-event.csv")))
readr::write_csv(IPT.occurrence, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-occurrence.csv")))
readr::write_csv(IPT.mof, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-mof.csv")))
## Analysis file
readr::write_csv(DF.dataWide, path = file.path(baseDir,baseAnalysis,paste0(rootFileName, "_analysis.csv")))
rootFileName = paste(unique(DF.sites$countryCodeISO), paste0(unique(DF.sites$localityCode, collapse="-")),
unique(DF.sites$HABITAT), gsub("-","", min(DF.sites$eventDate)), sep="_")
## IPT files
readr::write_csv(IPT.event, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-event.csv")))
readr::write_csv(IPT.occurrence, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-occurrence.csv")))
readr::write_csv(IPT.mof, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-mof.csv")))
## Analysis file
readr::write_csv(DF.dataWide, path = file.path(baseDir,baseAnalysis,paste0(rootFileName, "_analysis.csv")))
readr::write_csv(DF.sites, path = file.path(baseDir,baseAnalysis,paste0(rootFileName, "_site.csv")))
View(DF.sites)
View(DF.sites)
# Create a "IPT" and "Analysis" folder in your selected directory. Set your directory path as needed
baseDir = "."
baseIPT = "IPT"
baseAnalysis = "Analysis"
#setwd("~/P2P-templates")
# Select your data table
fileName = "DataSheet_longformat_TEST.xlsx"
## Extract information about your sampling site from the SiteInfo tab of your data table file
library(readxl)
DF.sites = read_xlsx(file.path(baseDir, fileName),sheet = "SiteInfo")
DF.sites = DF.sites[!is.na(DF.sites$COUNTRY),]
library(lutz)
library(lubridate)
library(knitr)
## get number of seconds from midnight
secsSTART = (1 -abs(as.numeric(julian(DF.sites$TIME_START)) - (as.integer(julian(DF.sites$TIME_START))))) * (60*60*24)
secsEND = (1 - abs(as.numeric(julian(DF.sites$TIME_END)) - (as.integer(julian(DF.sites$TIME_END))))) * (60*60*24)
dateChar = paste(DF.sites$YEAR, DF.sites$MONTH, DF.sites$DAY, sep="-")
## get timezone and timezone offset
timeZone = tz_lookup_coords(mean(DF.sites$LATITUDE, na.rm=T), mean(DF.sites$LONGITUDE, na.rm=T), method="accurate")
dateOffset = tz_offset(dateChar, timeZone)$utc_offset_h
## create data and time UTC
DF.sites$eventDate = as.POSIXct(dateChar, tz="UTC")
DF.sites$TIME_START = DF.sites$eventDate + seconds(secsSTART) + hours(dateOffset)
DF.sites$TIME_END = DF.sites$eventDate + seconds(secsEND) + hours(dateOffset)
DF.sites$eventTime = paste(format(DF.sites$TIME_START, "%H:%M:%SZ"), format(DF.sites$TIME_END, "%H:%M:%SZ"), sep="/")
print(timeZone)
kable(DF.sites[1:5, c(3:5, 13:14)])
library(countrycode)
# Country code
DF.sites$datasetName = paste0("MBON-P2P-biodiversity-",unique(DF.sites$countryCode))
DF.sites$countryCodeISO = countrycode(DF.sites$COUNTRY, "country.name","iso3c")
# Sampling protocol
DF.sites$samplingProtocol = "MBON-P2P_bestpractices-rockyshores"
# Sampling size value
DF.sites$samplingSizeValue = 0.25
# Sampling unit
DF.sites$samplingSizeUnit = "square meter"
print(DF.sites$countryCodeISO[1])
## data
DF.data = read_xlsx(file.path(baseDir, fileName),sheet = "DATA")
DF.data = DF.data[!is.na(DF.data$LOCALITY),]
## spp list
DF.spp = read_xlsx(file.path(baseDir, fileName),sheet = "sppList")
## codes
DF.countryCodes = read_xlsx(file.path(baseDir, fileName),sheet = "Countries")
DF.localityCodes = read_xlsx(file.path(baseDir, fileName),sheet = "Locality")
DF.siteCodes = read_xlsx(file.path(baseDir, fileName),sheet = "Sites")
DF.habitatCodes = read_xlsx(file.path(baseDir, fileName),sheet = "Habitat")
kable(DF.data[1:5, 1:7])
kable(DF.spp[1:5,])
library(dplyr)
## add codes: SITES
DF.sites = left_join(DF.sites, DF.countryCodes, by = "COUNTRY")
DF.sites = left_join(DF.sites, DF.localityCodes, by = "LOCALITY")
DF.sites = left_join(DF.sites, DF.siteCodes, by = "SITE")
DF.sites = left_join(DF.sites, DF.habitatCodes, by = "HABITAT")
DF.sites$PARENT_UNIT_ID = paste(DF.sites$countryCode, DF.sites$localityCode, DF.sites$siteCode, DF.sites$habitatCode,
paste0(DF.sites$YEAR, DF.sites$MONTH, DF.sites$DAY), sep="_")
DF.sites$UNIT_ID = paste(DF.sites$PARENT_UNIT_ID, DF.sites$STRATA, sep="_")
print(DF.sites$UNIT_ID[1:6])
## Add Aphia ID and taxa rank
DF.data = left_join(DF.data, DF.spp[,c("scientificName", "AphiaID", "Rank")])
DF.data = left_join(DF.data, DF.sites[,c("UNIT_ID", "LOCALITY", "SITE", "STRATA")])
DF.data = DF.data %>% group_by(LOCALITY, SITE, STRATA, SAMPLE) %>%
mutate(sampleOrganismID = 1:n(), scientificName, AphiaID, Rank, Variable, Value)
DF.data$occurrenceID = paste(DF.data$UNIT_ID, DF.data$SAMPLE, sprintf("%03d", DF.data$sampleOrganismID), sep="_")
print(DF.data$occurrenceID[1:20])
## to count per square meter
DF.data$Value[DF.data$Variable=="ABUNDANCE"] = DF.data$Value[DF.data$Variable=="ABUNDANCE"] * 4
print(DF.data$Value[DF.data$Variable=="ABUNDANCE"][1:10])
DF.data$basisOfRecord = "HumanObservation"
DF.data$occurrenceStatus = "present"
DF.data$scientificNameID = paste0("lsid:marinespecies.org:taxname:", DF.data$AphiaID)
## fields for the eMoF
DF.data$measurementTypeID = ifelse(DF.data$Variable=="COVER",
"http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL10/",  ##Coverage (in assayed sample) of biological entity
"http://vocab.nerc.ac.uk/collection/P06/current/UPMS/") ## number per square meter
DF.data$measurementUnit = ifelse(DF.data$Variable=="COVER", "percent", "count")
DF.data$measurementUnitID = ifelse(DF.data$Variable=="COVER",
"http://vocab.nerc.ac.uk/collection/P06/current/UPCT/",    ## percentage
"http://vocab.nerc.ac.uk/collection/P06/current/UPMS/")   ## number per square meter
DF.data = DF.data %>% arrange(occurrenceID, scientificName)
kable(DF.data[1:5, ])
## EventCore file
IPT.event = DF.sites %>%
select(datasetName,
parentEventID=PARENT_UNIT_ID,
eventID = UNIT_ID,
samplingProtocol,
samplingSizeValue,
samplingSizeUnit,
eventDate,
eventTime,
year = YEAR,
month = MONTH,
day = DAY,
habitat = HABITAT,
eventRemarks = REMARKS,
country = COUNTRY,
countryCode = countryCodeISO,
locality = LOCALITY,
decimalLatitude = LATITUDE,
decimalLongitude = LONGITUDE,
coordinateUncertaintyInMeters = GPS_ERROR,
geodeticDatum = DATUM,
strata=STRATA)
## Remove substrate type records
DF.data.noSubstrate = DF.data %>%
filter(! grepl("substrate", scientificName, fixed = T))
## OccurrenceCore file
IPT.occurrence = DF.data.noSubstrate %>% ungroup() %>%
select(eventID = UNIT_ID,
basisOfRecord,
occurrenceID,
scientificNameID,
scientificName,
taxonRank = Rank)
## Event Measurement or Fact (eMOF) file
IPT.mof = data.frame(eventID = DF.data.noSubstrate$UNIT_ID,
occurrenceID = DF.data.noSubstrate$occurrenceID,
measurementType = tolower(DF.data.noSubstrate$Variable),
measurmenetTypeID = DF.data.noSubstrate$measurementTypeID,
measurementValue = DF.data.noSubstrate$Value,
measurementUnit = DF.data.noSubstrate$measurementUnit,
measurementUnitID = DF.data.noSubstrate$measurementUnitID
)
print("EventCore")
kable(IPT.event[1:3, ])
print("OccurrenceCore")
kable(IPT.occurrence[1:3, ])
print("MoF")
kable(IPT.mof[1:3, ])
library(reshape2)
## reformat to wide
DF.dataWide = dcast(occurrenceID+LOCALITY+SITE+STRATA+SAMPLE+scientificName+AphiaID+Rank~Variable, value.var = "Value", data=DF.data, sum)
rootFileName = paste(unique(DF.sites$countryCodeISO), paste0(unique(DF.sites$localityCode, collapse="-")),
unique(DF.sites$HABITAT), gsub("-","", min(DF.sites$eventDate)), sep="_")
## IPT files
readr::write_csv(IPT.event, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-event.csv")))
readr::write_csv(IPT.occurrence, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-occurrence.csv")))
readr::write_csv(IPT.mof, path = file.path(baseDir,baseIPT,paste0(rootFileName, "_IPT-mof.csv")))
## Analysis file
readr::write_csv(DF.dataWide, path = file.path(baseDir,baseAnalysis,paste0(rootFileName, "_analysis.csv")))
readr::write_csv(DF.sites, path = file.path(baseDir,baseAnalysis,paste0(rootFileName, "_site.csv")))
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(lutz)
library(countrycode)
library(readxl)
library(reshape2)
library(lubridate)
library(dplyr)
library(ggplot2)
library(kableExtra)
options(dplyr.summarise.inform = FALSE)
# Create a "Data", IPT" and "Analysis" folder under your selected directory
baseDataDir = "Data"
baseIPT = "IPT"
baseAnalysis = "Analysis"
# Select your data table
fileName = "DataSheet_longformat_TEST.xlsx"
## Extract information about your sampling site from the SiteInfo tab of your data table file
DF.sites = read_xlsx(file.path(baseDataDir, fileName),sheet = "SiteInfo")
setwd("~/P2P-templates")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(lutz)
library(countrycode)
library(readxl)
library(reshape2)
library(lubridate)
library(dplyr)
library(ggplot2)
library(kableExtra)
options(dplyr.summarise.inform = FALSE)
# Set working directory
setwd("~/P2P-templates")
# Create a "Data", IPT" and "Analysis" folder under your selected directory
baseDataDir = "Data"
baseIPT = "IPT"
baseAnalysis = "Analysis"
# Select your data table
fileName = "DataSheet_longformat_TEST.xlsx"
## Extract information about your sampling site from the SiteInfo tab of your data table file
DF.sites = read_xlsx(file.path(baseDataDir, fileName),sheet = "SiteInfo")
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(lutz)
library(countrycode)
library(readxl)
library(reshape2)
library(lubridate)
library(dplyr)
library(ggplot2)
library(kableExtra)
options(dplyr.summarise.inform = FALSE)
# Create a "Data", IPT" and "Analysis" folder under your selected directory
baseDataDir = "Data"
baseIPT = "IPT"
baseAnalysis = "Analysis"
# Select your data table
fileName = "DataSheet_longformat_TEST.xlsx"
## Extract information about your sampling site from the SiteInfo tab of your data table file
DF.sites = read_xlsx(file.path(baseDataDir, fileName),sheet = "SiteInfo")
DF.sites = DF.sites[!is.na(DF.sites$COUNTRY),]
## get number of seconds from midnight
secsSTART = (1 -abs(as.numeric(julian(DF.sites$TIME_START)) - (as.integer(julian(DF.sites$TIME_START))))) * (60*60*24)
secsEND = (1 - abs(as.numeric(julian(DF.sites$TIME_END)) - (as.integer(julian(DF.sites$TIME_END))))) * (60*60*24)
dateChar = paste(DF.sites$YEAR, DF.sites$MONTH, DF.sites$DAY, sep="-")
## get timezone and timezone offset
timeZone = tz_lookup_coords(mean(DF.sites$LATITUDE, na.rm=T), mean(DF.sites$LONGITUDE, na.rm=T), method="accurate")
dateOffset = tz_offset(dateChar, timeZone)$utc_offset_h
## create data and time UTC
DF.sites$eventDate = as.POSIXct(dateChar, tz="UTC")
DF.sites$TIME_START = DF.sites$eventDate + seconds(secsSTART) + hours(dateOffset)
DF.sites$TIME_END = DF.sites$eventDate + seconds(secsEND) + hours(dateOffset)
DF.sites$eventTime = paste(format(DF.sites$TIME_START, "%H:%M:%SZ"), format(DF.sites$TIME_END, "%H:%M:%SZ"), sep="/")
print(timeZone)
kable(DF.sites[1:5, c(3:5, 13:14)]) %>% kable_styling("striped")
# Country code
DF.sites$datasetName = paste0("MBON-P2P-biodiversity-",unique(DF.sites$countryCode))
DF.sites$countryCodeISO = countrycode(DF.sites$COUNTRY, "country.name","iso3c")
# Sampling protocol
DF.sites$samplingProtocol = "MBON-P2P_bestpractices-rockyshores"
# Sampling size value
DF.sites$samplingSizeValue = 0.25
# Sampling unit
DF.sites$samplingSizeUnit = "square meter"
print(DF.sites$countryCodeISO[1])
## data
DF.data = read_xlsx(file.path(baseDataDir, fileName),sheet = "DATA")
DF.data = DF.data[!is.na(DF.data$LOCALITY),]
## spp list
DF.spp = read_xlsx(file.path(baseDataDir, fileName),sheet = "sppList")
## codes
DF.countryCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Countries")
DF.localityCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Locality")
DF.siteCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Sites")
DF.habitatCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Habitat")
kable(DF.data[1:5, 1:7]) %>% kable_styling("striped")
kable(DF.spp[1:5,]) %>% kable_styling("striped")
## add codes: SITES
DF.sites = left_join(DF.sites, DF.countryCodes, by = "COUNTRY")
DF.sites = left_join(DF.sites, DF.localityCodes, by = "LOCALITY")
DF.sites = left_join(DF.sites, DF.siteCodes, by = "SITE")
DF.sites = left_join(DF.sites, DF.habitatCodes, by = "HABITAT")
DF.sites$PARENT_UNIT_ID = paste(DF.sites$countryCode, DF.sites$localityCode, DF.sites$siteCode, DF.sites$habitatCode,
paste0(DF.sites$YEAR, DF.sites$MONTH, DF.sites$DAY), sep="_")
DF.sites$UNIT_ID = paste(DF.sites$PARENT_UNIT_ID, DF.sites$STRATA, sep="_")
print(DF.sites$UNIT_ID[1:6])
## Add Aphia ID and taxa rank
DF.data = left_join(DF.data, DF.spp[,c("scientificName", "AphiaID", "Rank")])
DF.data = left_join(DF.data, DF.sites[,c("UNIT_ID", "LOCALITY", "SITE", "STRATA")])
DF.data = DF.data %>% group_by(LOCALITY, SITE, STRATA, SAMPLE) %>%
mutate(sampleOrganismID = 1:n(), scientificName, AphiaID, Rank, Variable, Value)
DF.data$occurrenceID = paste(DF.data$UNIT_ID, DF.data$SAMPLE, sprintf("%03d", DF.data$sampleOrganismID), sep="_")
print(DF.data$occurrenceID[1:20])
## to count per square meter
DF.data$Value[DF.data$Variable=="ABUNDANCE"] = DF.data$Value[DF.data$Variable=="ABUNDANCE"] * 4
print(DF.data$Value[DF.data$Variable=="ABUNDANCE"][1:10])
DF.data$basisOfRecord = "HumanObservation"
DF.data$occurrenceStatus = "present"
DF.data$scientificNameID = paste0("lsid:marinespecies.org:taxname:", DF.data$AphiaID)
## fields for the eMoF
DF.data$measurementTypeID = ifelse(DF.data$Variable=="COVER",
"http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL10/",  ##Coverage (in assayed sample) of biological entity
"http://vocab.nerc.ac.uk/collection/P06/current/UPMS/") ## number per square meter
DF.data$measurementUnit = ifelse(DF.data$Variable=="COVER", "percent", "count")
DF.data$measurementUnitID = ifelse(DF.data$Variable=="COVER",
"http://vocab.nerc.ac.uk/collection/P06/current/UPCT/",    ## percentage
"http://vocab.nerc.ac.uk/collection/P06/current/UPMS/")   ## number per square meter
DF.data = DF.data %>% arrange(occurrenceID, scientificName)
kable(DF.data[1:5, ]) %>% kable_styling("striped")
## EventCore file
IPT.event = DF.sites %>%
select(datasetName,
parentEventID=PARENT_UNIT_ID,
eventID = UNIT_ID,
samplingProtocol,
samplingSizeValue,
samplingSizeUnit,
eventDate,
eventTime,
year = YEAR,
month = MONTH,
day = DAY,
habitat = HABITAT,
eventRemarks = REMARKS,
country = COUNTRY,
countryCode = countryCodeISO,
locality = LOCALITY,
decimalLatitude = LATITUDE,
decimalLongitude = LONGITUDE,
coordinateUncertaintyInMeters = GPS_ERROR,
geodeticDatum = DATUM,
strata=STRATA)
## Remove substrate type records
DF.data.noSubstrate = DF.data %>%
filter(! grepl("substrate", scientificName, fixed = T))
## OccurrenceCore file
IPT.occurrence = DF.data.noSubstrate %>% ungroup() %>%
select(eventID = UNIT_ID,
basisOfRecord,
occurrenceID,
scientificNameID,
scientificName,
taxonRank = Rank)
## Event Measurement or Fact (eMOF) file
IPT.mof = data.frame(eventID = DF.data.noSubstrate$UNIT_ID,
occurrenceID = DF.data.noSubstrate$occurrenceID,
measurementType = tolower(DF.data.noSubstrate$Variable),
measurmenetTypeID = DF.data.noSubstrate$measurementTypeID,
measurementValue = DF.data.noSubstrate$Value,
measurementUnit = DF.data.noSubstrate$measurementUnit,
measurementUnitID = DF.data.noSubstrate$measurementUnitID
)
print("EventCore")
kable(IPT.event[1:3, ]) %>% kable_styling("striped")
print("OccurrenceCore")
kable(IPT.occurrence[1:3, ]) %>% kable_styling("striped")
print("MoF")
kable(IPT.mof[1:3, ]) %>% kable_styling("striped")
## reformat to wide
DF.dataWide = dcast(occurrenceID+LOCALITY+SITE+STRATA+SAMPLE+scientificName+AphiaID+Rank~Variable, value.var = "Value", data=DF.data, sum)
rootFileName = paste(unique(DF.sites$countryCodeISO), paste0(unique(DF.sites$localityCode, collapse="-")),
unique(DF.sites$HABITAT), gsub("-","", min(DF.sites$eventDate)), sep="_")
## IPT files
readr::write_csv(IPT.event, path = file.path(baseIPT,paste0(rootFileName, "_IPT-event.csv")))
readr::write_csv(IPT.occurrence, path = file.path(baseIPT,paste0(rootFileName, "_IPT-occurrence.csv")))
readr::write_csv(IPT.mof, path = file.path(baseIPT,paste0(rootFileName, "_IPT-mof.csv")))
## Analysis file
readr::write_csv(DF.dataWide, path = file.path(baseAnalysis,paste0(rootFileName, "_analysis.csv")))
readr::write_csv(DF.sites, path = file.path(baseAnalysis,paste0(rootFileName, "_site.csv")))
setwd("~/P2P-templates")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(lutz)
library(countrycode)
library(readxl)
library(reshape2)
library(lubridate)
library(dplyr)
library(ggplot2)
library(kableExtra)
options(dplyr.summarise.inform = FALSE)
# Create a "Data", IPT" and "Analysis" folder under your selected directory
baseDataDir = "Data"
baseIPT = "IPT"
baseAnalysis = "Analysis"
# Select your data table
fileName = "DataSheet_longformat_TEST_v2.xlsx"
## Extract information about your sampling site from the SiteInfo tab of your data table file
DF.sites = read_xlsx(file.path(baseDataDir, fileName),sheet = "SiteInfo")
DF.sites = DF.sites[!is.na(DF.sites$COUNTRY),]
## get number of seconds from midnight
secsSTART = (1 -abs(as.numeric(julian(DF.sites$TIME_START)) - (as.integer(julian(DF.sites$TIME_START))))) * (60*60*24)
secsEND = (1 - abs(as.numeric(julian(DF.sites$TIME_END)) - (as.integer(julian(DF.sites$TIME_END))))) * (60*60*24)
dateChar = paste(DF.sites$YEAR, DF.sites$MONTH, DF.sites$DAY, sep="-")
## get timezone and timezone offset
timeZone = tz_lookup_coords(mean(DF.sites$LATITUDE, na.rm=T), mean(DF.sites$LONGITUDE, na.rm=T), method="accurate")
dateOffset = tz_offset(dateChar, timeZone)$utc_offset_h
## create data and time UTC
DF.sites$eventDate = as.POSIXct(dateChar, tz="UTC")
DF.sites$TIME_START = DF.sites$eventDate + seconds(secsSTART) + hours(dateOffset)
DF.sites$TIME_END = DF.sites$eventDate + seconds(secsEND) + hours(dateOffset)
DF.sites$eventTime = paste(format(DF.sites$TIME_START, "%H:%M:%SZ"), format(DF.sites$TIME_END, "%H:%M:%SZ"), sep="/")
print(timeZone)
kable(DF.sites[1:5, c(3:5, 13:14)]) %>% kable_styling("striped")
# Country code
DF.sites$datasetName = paste0("MBON-P2P-biodiversity-",unique(DF.sites$countryCode))
DF.sites$countryCodeISO = countrycode(DF.sites$COUNTRY, "country.name","iso3c")
# Sampling protocol
DF.sites$samplingProtocol = "MBON-P2P_bestpractices-rockyshores"
# Sampling size value
DF.sites$samplingSizeValue = 0.25
# Sampling unit
DF.sites$samplingSizeUnit = "square meter"
print(DF.sites$countryCodeISO[1])
## data
DF.data = read_xlsx(file.path(baseDataDir, fileName),sheet = "DATA")
DF.data = DF.data[!is.na(DF.data$LOCALITY),]
## spp list
DF.spp = read_xlsx(file.path(baseDataDir, fileName),sheet = "sppList")
## codes
DF.countryCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Countries")
DF.localityCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Locality")
DF.siteCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Sites")
DF.habitatCodes = read_xlsx(file.path(baseDataDir, fileName),sheet = "Habitat")
kable(DF.data[1:5, 1:7]) %>% kable_styling("striped")
kable(DF.spp[1:5,]) %>% kable_styling("striped")
## add codes: SITES
DF.sites = left_join(DF.sites, DF.countryCodes, by = "COUNTRY")
DF.sites = left_join(DF.sites, DF.localityCodes, by = "LOCALITY")
DF.sites = left_join(DF.sites, DF.siteCodes, by = "SITE")
DF.sites = left_join(DF.sites, DF.habitatCodes, by = "HABITAT")
DF.sites$PARENT_UNIT_ID = paste(DF.sites$countryCode, DF.sites$localityCode, DF.sites$siteCode, DF.sites$habitatCode,
paste0(DF.sites$YEAR, DF.sites$MONTH, DF.sites$DAY), sep="_")
DF.sites$UNIT_ID = paste(DF.sites$PARENT_UNIT_ID, DF.sites$STRATA, sep="_")
print(DF.sites$UNIT_ID[1:6])
## Add Aphia ID and taxa rank
DF.data = left_join(DF.data, DF.spp[,c("scientificName", "AphiaID", "Rank")])
DF.data = left_join(DF.data, DF.sites[,c("UNIT_ID", "LOCALITY", "SITE", "STRATA")])
DF.data = DF.data %>% group_by(LOCALITY, SITE, STRATA, SAMPLE) %>%
mutate(sampleOrganismID = 1:n(), scientificName, AphiaID, Rank, Variable, Value)
DF.data$occurrenceID = paste(DF.data$UNIT_ID, DF.data$SAMPLE, sprintf("%03d", DF.data$sampleOrganismID), sep="_")
print(DF.data$occurrenceID[1:20])
## to count per square meter
DF.data$Value[DF.data$Variable=="ABUNDANCE"] = DF.data$Value[DF.data$Variable=="ABUNDANCE"] * 4
print(DF.data$Value[DF.data$Variable=="ABUNDANCE"][1:10])
DF.data$basisOfRecord = "HumanObservation"
DF.data$occurrenceStatus = "present"
DF.data$scientificNameID = paste0("lsid:marinespecies.org:taxname:", DF.data$AphiaID)
## fields for the eMoF
DF.data$measurementTypeID = ifelse(DF.data$Variable=="COVER",
"http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL10/",  ##Coverage (in assayed sample) of biological entity
"http://vocab.nerc.ac.uk/collection/P06/current/UPMS/") ## number per square meter
DF.data$measurementUnit = ifelse(DF.data$Variable=="COVER", "percent", "count")
DF.data$measurementUnitID = ifelse(DF.data$Variable=="COVER",
"http://vocab.nerc.ac.uk/collection/P06/current/UPCT/",    ## percentage
"http://vocab.nerc.ac.uk/collection/P06/current/UPMS/")   ## number per square meter
DF.data = DF.data %>% arrange(occurrenceID, scientificName)
kable(DF.data[1:5, ]) %>% kable_styling("striped")
## EventCore file
IPT.event = DF.sites %>%
select(datasetName,
parentEventID=PARENT_UNIT_ID,
eventID = UNIT_ID,
samplingProtocol,
samplingSizeValue,
samplingSizeUnit,
eventDate,
eventTime,
year = YEAR,
month = MONTH,
day = DAY,
habitat = HABITAT,
eventRemarks = REMARKS,
country = COUNTRY,
countryCode = countryCodeISO,
locality = LOCALITY,
decimalLatitude = LATITUDE.x,
decimalLongitude = LONGITUDE.x,
coordinateUncertaintyInMeters = GPS_ERROR,
geodeticDatum = DATUM,
strata=STRATA)
## Remove substrate type records
DF.data.noSubstrate = DF.data %>%
filter(! grepl("substrate", scientificName, fixed = T))
## OccurrenceCore file
IPT.occurrence = DF.data.noSubstrate %>% ungroup() %>%
select(eventID = UNIT_ID,
basisOfRecord,
occurrenceID,
scientificNameID,
scientificName,
taxonRank = Rank)
## Event Measurement or Fact (eMOF) file
IPT.mof = data.frame(eventID = DF.data.noSubstrate$UNIT_ID,
occurrenceID = DF.data.noSubstrate$occurrenceID,
measurementType = tolower(DF.data.noSubstrate$Variable),
measurmenetTypeID = DF.data.noSubstrate$measurementTypeID,
measurementValue = DF.data.noSubstrate$Value,
measurementUnit = DF.data.noSubstrate$measurementUnit,
measurementUnitID = DF.data.noSubstrate$measurementUnitID
)
print("EventCore")
kable(IPT.event[1:3, ]) %>% kable_styling("striped")
print("OccurrenceCore")
kable(IPT.occurrence[1:3, ]) %>% kable_styling("striped")
print("MoF")
kable(IPT.mof[1:3, ]) %>% kable_styling("striped")
## reformat to wide
DF.dataWide = dcast(occurrenceID+LOCALITY+SITE+STRATA+SAMPLE+scientificName+AphiaID+Rank~Variable, value.var = "Value", data=DF.data, sum)
rootFileName = paste(unique(DF.sites$countryCodeISO), paste0(unique(DF.sites$localityCode, collapse="-")),
unique(DF.sites$HABITAT), gsub("-","", min(DF.sites$eventDate)), sep="_")
## IPT files
readr::write_csv(IPT.event, path = file.path(baseIPT,paste0(rootFileName, "_IPT-event.csv")))
readr::write_csv(IPT.occurrence, path = file.path(baseIPT,paste0(rootFileName, "_IPT-occurrence.csv")))
readr::write_csv(IPT.mof, path = file.path(baseIPT,paste0(rootFileName, "_IPT-mof.csv")))
## Analysis file
readr::write_csv(DF.dataWide, path = file.path(baseAnalysis,paste0(rootFileName, "_analysis.csv")))
readr::write_csv(DF.sites, path = file.path(baseAnalysis,paste0(rootFileName, "_site.csv")))
